{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章:ニューラルネット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 70.単語ベクトルの和による特徴量\n",
    "問題50で構築した学習データ，検証データ，評価データを行列・ベクトルに変換したい．例えば，学習データについて，すべての事例xi\n",
    "の特徴ベクトルxi\n",
    "を並べた行列X\n",
    "と，正解ラベルを並べた行列（ベクトル）Y\n",
    "を作成したい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【学習データ】\n",
      "b    4502\n",
      "e    4223\n",
      "t    1219\n",
      "m     728\n",
      "Name: CATEGORY, dtype: int64\n",
      "【検証データ】\n",
      "b    562\n",
      "e    528\n",
      "t    153\n",
      "m     91\n",
      "Name: CATEGORY, dtype: int64\n",
      "【評価データ】\n",
      "b    563\n",
      "e    528\n",
      "t    152\n",
      "m     91\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データの読込\n",
    "df = pd.read_csv('./NewsAggregatorDataset/newsCorpora.csv', header=None, sep='\\t', names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "# データの抽出\n",
    "df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail']), ['TITLE', 'CATEGORY']]\n",
    "\n",
    "# データの分割\n",
    "train, valid_test = train_test_split(df, test_size=0.2, shuffle=True, random_state=123, stratify=df['CATEGORY'])\n",
    "valid, test = train_test_split(valid_test, test_size=0.5, shuffle=True, random_state=123, stratify=valid_test['CATEGORY'])\n",
    "\n",
    "# データの保存\n",
    "train.to_csv('./train.txt', sep='\\t', index=False)\n",
    "valid.to_csv('./valid.txt', sep='\\t', index=False)\n",
    "test.to_csv('./test.txt', sep='\\t', index=False)\n",
    "\n",
    "# 事例数の確認\n",
    "print('【学習データ】')\n",
    "print(train['CATEGORY'].value_counts())\n",
    "print('【検証データ】')\n",
    "print(valid['CATEGORY'].value_counts())\n",
    "print('【評価データ】')\n",
    "print(test['CATEGORY'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# ダウンロードファイルのロード  #単語ベクトルを読み込む\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import torch\n",
    "\n",
    "def transform_w2v(text):    #string.punctuation='!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    words = text.translate(table).split()  # string.punctuationをスペースに置換後、スペースで分割してリスト化\n",
    "    vec = [model[word] for word in words if word in model]  # 1語ずつベクトル化\n",
    "\n",
    "    return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10672, 300])\n",
      "tensor([[-0.0170,  0.1318, -0.0728,  ...,  0.0395,  0.0223,  0.0184],\n",
      "        [-0.1119, -0.0523, -0.1002,  ...,  0.0319, -0.0237, -0.0425],\n",
      "        [-0.0636, -0.0228, -0.0005,  ..., -0.0280,  0.1057,  0.0396],\n",
      "        ...,\n",
      "        [ 0.0301, -0.0355, -0.0082,  ..., -0.0045,  0.0606, -0.0281],\n",
      "        [ 0.0002,  0.0442, -0.0424,  ..., -0.0507,  0.0283,  0.0365],\n",
      "        [ 0.0242,  0.0418,  0.1211,  ..., -0.0564,  0.0144,  0.0277]])\n"
     ]
    }
   ],
   "source": [
    "# 特徴ベクトルの作成\n",
    "X_train = torch.stack([transform_w2v(text) for text in train['TITLE']])\n",
    "X_valid = torch.stack([transform_w2v(text) for text in valid['TITLE']])\n",
    "X_test = torch.stack([transform_w2v(text) for text in test['TITLE']])\n",
    "\n",
    "print(X_train.size())\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10672])\n",
      "tensor([0, 0, 0,  ..., 0, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# ラベルベクトルの作成\n",
    "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
    "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "\n",
    "print(y_train.size())\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "torch.save(X_train, 'X_train.pt')\n",
    "torch.save(X_valid, 'X_valid.pt')\n",
    "torch.save(X_test, 'X_test.pt')\n",
    "torch.save(y_train, 'y_train.pt')\n",
    "torch.save(y_valid, 'y_valid.pt')\n",
    "torch.save(y_test, 'y_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 71.単層ニューラルネットワークによる予測\n",
    "問題70で保存した行列を読み込み，学習データについて以下の計算を実行せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SLPNet(nn.Module): #単層ニューラルネットワークを定義\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size, bias=False)   #ネットワーク層\n",
    "        nn.init.normal_(self.fc.weight, 0.0, 1.0)  # 正規乱数で重みを初期化\n",
    "\n",
    "    def forward(self, x):   #順伝播で通るレイヤーを配置\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4])\n",
      "tensor([[0.5499, 0.4140, 0.0216, 0.0145]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = SLPNet(300, 4)  # 単層ニューラルネットワークの初期化   #定義したモデルを初期化\n",
    "y_hat_1 = torch.softmax(model(X_train[:1]), dim=-1)      #題の計算\n",
    "print(y_hat_1.size())\n",
    "print(y_hat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "tensor([[0.1147, 0.8330, 0.0075, 0.0449],\n",
      "        [0.5347, 0.4454, 0.0031, 0.0168],\n",
      "        [0.4620, 0.1664, 0.0440, 0.3275],\n",
      "        [0.1963, 0.5527, 0.0687, 0.1822]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Y_hat = torch.softmax(model.forward(X_train[:4]), dim=-1)   #dim=-1 最後の次元の要素の和が1になる\n",
    "print(Y_hat.size())\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 72.損失と勾配の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失: 0.5981\n",
      "勾配:\n",
      "tensor([[ 0.0076, -0.0593,  0.0327,  ..., -0.0178, -0.0100, -0.0083],\n",
      "        [-0.0070,  0.0546, -0.0301,  ...,  0.0164,  0.0092,  0.0076],\n",
      "        [-0.0004,  0.0028, -0.0016,  ...,  0.0009,  0.0005,  0.0004],\n",
      "        [-0.0002,  0.0019, -0.0011,  ...,  0.0006,  0.0003,  0.0003]])\n",
      "-------------------------------\n",
      "tensor([[ 1.8965,  1.6128, -1.3407, -1.7408]], grad_fn=<MmBackward0>)\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "l_1 = criterion(model(X_train[:1]), y_train[:1])  # 入力ベクトルはsoftmax前の値\n",
    "model.zero_grad()  # 勾配をゼロで初期化\n",
    "l_1.backward()  # 勾配を計算\n",
    "print(f'損失: {l_1:.4f}')\n",
    "print(f'勾配:\\n{model.fc.weight.grad}')\n",
    "print(\"-------------------------------\")\n",
    "print(model(X_train[:1]))\n",
    "print(y_train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失: 1.6214\n",
      "勾配:\n",
      "tensor([[ 0.0179, -0.0496,  0.0236,  ..., -0.0096, -0.0107, -0.0061],\n",
      "        [-0.0028,  0.0080, -0.0007,  ..., -0.0002,  0.0016,  0.0011],\n",
      "        [-0.0096,  0.0319, -0.0192,  ...,  0.0091,  0.0060,  0.0036],\n",
      "        [-0.0054,  0.0097, -0.0037,  ...,  0.0008,  0.0031,  0.0014]])\n"
     ]
    }
   ],
   "source": [
    "l = criterion(model(X_train[:4]), y_train[:4])\n",
    "model.zero_grad()\n",
    "l.backward()\n",
    "print(f'損失: {l:.4f}')\n",
    "print(f'勾配:\\n{model.fc.weight.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 73.確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，行列W\n",
    "を学習せよ．なお，学習は適当な基準で終了させればよい（例えば「100エポックで終了」など）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 特徴ベクトルとラベルベクトルを合わせて保持することができる型\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):  # datasetの構成要素を指定\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):  # len(dataset)で返す値を指定\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):  # dataset[idx]で返す値を指定\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Datasetの作成\n",
    "dataset_train = NewsDataset(X_train, y_train)\n",
    "dataset_valid = NewsDataset(X_valid, y_valid)\n",
    "dataset_test = NewsDataset(X_test, y_test)\n",
    "\n",
    "# Dataloaderの作成\n",
    "#batch_sizeのデータを取り出す\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10671\n",
      "epoch: 1, loss_train: 0.4767, loss_valid: 0.3393\n",
      "10671\n",
      "epoch: 2, loss_train: 0.3181, loss_valid: 0.2980\n",
      "10671\n",
      "epoch: 3, loss_train: 0.2893, loss_valid: 0.2851\n",
      "10671\n",
      "epoch: 4, loss_train: 0.2738, loss_valid: 0.2786\n",
      "10671\n",
      "epoch: 5, loss_train: 0.2632, loss_valid: 0.2692\n",
      "10671\n",
      "epoch: 6, loss_train: 0.2562, loss_valid: 0.2682\n",
      "10671\n",
      "epoch: 7, loss_train: 0.2511, loss_valid: 0.2647\n",
      "10671\n",
      "epoch: 8, loss_train: 0.2463, loss_valid: 0.2650\n",
      "10671\n",
      "epoch: 9, loss_train: 0.2428, loss_valid: 0.2660\n",
      "10671\n",
      "epoch: 10, loss_train: 0.2401, loss_valid: 0.2645\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義 #最適化 #勾配計算\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)    #確率的勾配降下法\n",
    "\n",
    "# 学習\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "        # 勾配をゼロで初期化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 損失を記録\n",
    "        loss_train += loss.item()\n",
    "        print\n",
    "\n",
    "    # バッチ単位の平均損失計算   #1epochごと?\n",
    "    loss_train = loss_train / i\n",
    "    #print(i)\n",
    "\n",
    "    # 検証データの損失計算\n",
    "    model.eval() \n",
    "    with torch.no_grad():   #検証なので学習時とは違い勾配計算不要、テンソルの勾配計算を不可にしてメモリの消費を減らす\n",
    "        inputs, labels = next(iter(dataloader_valid))\n",
    "        outputs = model(inputs)\n",
    "        loss_valid = criterion(outputs, labels)\n",
    "\n",
    "    # ログを出力\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 74.正解率の計測\n",
    "問題73で求めた行列を用いて学習データおよび評価データの事例を分類したとき，その正解率をそれぞれ求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:  #dataloaderから\n",
    "            outputs = model(inputs)\n",
    "            pred = torch.argmax(outputs, dim=-1)    #予測ラベル\n",
    "            #print(outputs)\n",
    "            total += len(inputs)    #母数\n",
    "            correct += (pred == labels).sum().item()\n",
    "            #print(pred)\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率（学習データ）：0.920\n",
      "正解率（評価データ）：0.911\n"
     ]
    }
   ],
   "source": [
    "acc_train = calculate_accuracy(model, dataloader_train)\n",
    "acc_test = calculate_accuracy(model, dataloader_test)\n",
    "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
    "print(f'正解率（評価データ）：{acc_test:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
