{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第8章: ニューラルネット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 70. 単語ベクトルの和による特徴量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('./NewsAggregatorDataset/train.txt', sep='\\t', names=['CATEGORY', 'TITLE'])\n",
    "valid = pd.read_csv('./NewsAggregatorDataset/valid.txt', sep='\\t', names=['CATEGORY', 'TITLE'])\n",
    "test = pd.read_csv('./NewsAggregatorDataset/test.txt', sep='\\t', names=['CATEGORY', 'TITLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10672, 300])\n",
      "tensor([[ 0.0930,  0.1105, -0.0617,  ...,  0.0472,  0.1016,  0.1064],\n",
      "        [ 0.0773,  0.1216, -0.0084,  ...,  0.0125,  0.0744,  0.0549],\n",
      "        [ 0.0144, -0.0348,  0.0461,  ...,  0.0654,  0.0590, -0.0838],\n",
      "        ...,\n",
      "        [-0.0356,  0.1175,  0.1018,  ..., -0.0172,  0.0627,  0.0960],\n",
      "        [ 0.0226,  0.0159, -0.0263,  ...,  0.0301,  0.1126, -0.1359],\n",
      "        [-0.0124, -0.0050, -0.0422,  ..., -0.0068,  0.1063, -0.0074]])\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import torch\n",
    "\n",
    "def transform_w2v(text):\n",
    "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    words = text.translate(table).split()  # 記号をスペースに置換後、スペースで分割してリスト化\n",
    "    vec = [model[word] for word in words if word in model]  # 1語ずつベクトル化\n",
    "\n",
    "    return torch.tensor(sum(vec) / len(vec))  # 平均ベクトルをTensor型に変換して出力\n",
    "\n",
    "# 特徴ベクトルの作成\n",
    "X_train = torch.stack([transform_w2v(text) for text in train['TITLE']])\n",
    "X_valid = torch.stack([transform_w2v(text) for text in valid['TITLE']])\n",
    "X_test = torch.stack([transform_w2v(text) for text in test['TITLE']])\n",
    "\n",
    "print(X_train.size())\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10672])\n",
      "tensor([2, 0, 0,  ..., 0, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "# ラベルベクトルの作成\n",
    "category_dict = {'b': 0, 't': 1, 'e':2, 'm':3}\n",
    "y_train = torch.tensor(train['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "y_valid = torch.tensor(valid['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "y_test = torch.tensor(test['CATEGORY'].map(lambda x: category_dict[x]).values)\n",
    "\n",
    "print(y_train.size())\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "torch.save(X_train, 'X_train.pt')\n",
    "torch.save(X_valid, 'X_valid.pt')\n",
    "torch.save(X_test, 'X_test.pt')\n",
    "torch.save(y_train, 'y_train.pt')\n",
    "torch.save(y_valid, 'y_valid.pt')\n",
    "torch.save(y_test, 'y_test.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 71. 単層ニューラルネットワークによる予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0048, 0.0951, 0.8782, 0.0219]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class SLPNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size, bias=False)\n",
    "        nn.init.normal_(self.fc.weight, 0.0, 1.0)  # 正規乱数で重みを初期化\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "model = SLPNet(300, 4)  # 単層ニューラルネットワークの初期化\n",
    "y_hat_1 = torch.softmax(model(X_train[:1]), dim=-1)\n",
    "print(y_hat_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y_hat = torch.softmax(model.forward(X_train[:4]), dim=-1)\n",
    "# print(Y_hat)\n",
    "y_hat_2 = torch.softmax(model(X_train[1:2]), dim=-1)\n",
    "y_hat_3 = torch.softmax(model(X_train[2:3]), dim=-1)\n",
    "y_hat_4 = torch.softmax(model(X_train[3:4]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0048, 0.0951, 0.8782, 0.0219],\n",
      "        [0.4545, 0.0101, 0.5086, 0.0269],\n",
      "        [0.8323, 0.0195, 0.0826, 0.0656],\n",
      "        [0.0678, 0.0548, 0.7666, 0.1108]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "Y_hat = torch.softmax(model.forward(X_train[:4]), dim=-1)\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 72. 損失と勾配の計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失: 0.1299\n",
      "勾配:\n",
      "tensor([[ 0.0004,  0.0005, -0.0003,  ...,  0.0002,  0.0005,  0.0005],\n",
      "        [ 0.0088,  0.0105, -0.0059,  ...,  0.0045,  0.0097,  0.0101],\n",
      "        [-0.0113, -0.0135,  0.0075,  ..., -0.0057, -0.0124, -0.0130],\n",
      "        [ 0.0020,  0.0024, -0.0014,  ...,  0.0010,  0.0022,  0.0023]])\n"
     ]
    }
   ],
   "source": [
    "l_1 = criterion(model(X_train[:1]), y_train[:1])  # 入力ベクトルはsoftmax前の値\n",
    "model.zero_grad()  # 勾配をゼロで初期化\n",
    "l_1.backward()  # 勾配を計算\n",
    "print(f'損失: {l_1:.4f}')\n",
    "print(f'勾配:\\n{model.fc.weight.grad}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "損失: 0.3420\n",
      "勾配:\n",
      "tensor([[-0.0116, -0.0145, -0.0014,  ..., -0.0045, -0.0120, -0.0029],\n",
      "        [ 0.0020,  0.0032, -0.0017,  ...,  0.0014,  0.0033,  0.0031],\n",
      "        [ 0.0091,  0.0096,  0.0038,  ...,  0.0019,  0.0059, -0.0014],\n",
      "        [ 0.0004,  0.0017, -0.0006,  ...,  0.0012,  0.0028,  0.0012]])\n"
     ]
    }
   ],
   "source": [
    "l = criterion(model(X_train[:4]), y_train[:4])\n",
    "model.zero_grad()\n",
    "l.backward()\n",
    "print(f'損失: {l:.4f}')\n",
    "print(f'勾配:\\n{model.fc.weight.grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 73. 確率的勾配降下法による学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):  # datasetの構成要素を指定\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):  # len(dataset)で返す値を指定\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):  # dataset[idx]で返す値を指定\n",
    "        return [self.X[idx], self.y[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Datasetの作成\n",
    "dataset_train = NewsDataset(X_train, y_train)\n",
    "dataset_valid = NewsDataset(X_valid, y_valid)\n",
    "dataset_test = NewsDataset(X_test, y_test)\n",
    "\n",
    "# Dataloaderの作成\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=1, shuffle=True)\n",
    "dataloader_valid = DataLoader(dataset_valid, batch_size=len(dataset_valid), shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=len(dataset_test), shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss_train: 0.4858, loss_valid: 0.3636\n",
      "epoch: 2, loss_train: 0.3172, loss_valid: 0.3330\n",
      "epoch: 3, loss_train: 0.2856, loss_valid: 0.3139\n",
      "epoch: 4, loss_train: 0.2700, loss_valid: 0.3062\n",
      "epoch: 5, loss_train: 0.2603, loss_valid: 0.3025\n",
      "epoch: 6, loss_train: 0.2529, loss_valid: 0.3038\n",
      "epoch: 7, loss_train: 0.2470, loss_valid: 0.2976\n",
      "epoch: 8, loss_train: 0.2421, loss_valid: 0.3015\n",
      "epoch: 9, loss_train: 0.2395, loss_valid: 0.2973\n",
      "epoch: 10, loss_train: 0.2367, loss_valid: 0.2942\n"
     ]
    }
   ],
   "source": [
    "# モデルの定義\n",
    "model = SLPNet(300, 4)\n",
    "\n",
    "# 損失関数の定義\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# オプティマイザの定義\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "# 学習\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # 訓練モードに設定\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    for i, (inputs, labels) in enumerate(dataloader_train):\n",
    "        # 勾配をゼロで初期化\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 損失を記録\n",
    "        loss_train += loss.item()\n",
    "        \n",
    "    # バッチ単位の平均損失計算\n",
    "    loss_train = loss_train / i\n",
    "    # 検証データの損失計算\n",
    "    model.eval() \n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(dataloader_valid))\n",
    "        outputs = model(inputs)\n",
    "        loss_valid = criterion(outputs, labels)\n",
    "        \n",
    "    # ログを出力\n",
    "    print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f}, loss_valid: {loss_valid:.4f}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 74. 正解率の計測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, loader):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正解率（学習データ）：0.921\n",
      "正解率（評価データ）：0.903\n"
     ]
    }
   ],
   "source": [
    "acc_train = calculate_accuracy(model, dataloader_train)\n",
    "acc_test = calculate_accuracy(model, dataloader_test)\n",
    "print(f'正解率（学習データ）：{acc_train:.3f}')\n",
    "print(f'正解率（評価データ）：{acc_test:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
